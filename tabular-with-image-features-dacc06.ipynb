{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":63056,"databundleVersionId":9094797,"sourceType":"competition"},{"sourceId":8982084,"sourceType":"datasetVersion","datasetId":5406640},{"sourceId":8991790,"sourceType":"datasetVersion","datasetId":5415918},{"sourceId":186147615,"sourceType":"kernelVersion"},{"sourceId":186149019,"sourceType":"kernelVersion"},{"sourceId":187730674,"sourceType":"kernelVersion"},{"sourceId":188543089,"sourceType":"kernelVersion"},{"sourceId":188543756,"sourceType":"kernelVersion"},{"sourceId":188602899,"sourceType":"kernelVersion"},{"sourceId":188603204,"sourceType":"kernelVersion"},{"sourceId":188603902,"sourceType":"kernelVersion"}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook is copied from excellent work: thanks to https://www.kaggle.com/code/greysky/isic-2024-only-tabular-data\n\nI add image predictions to this notebook:\n\n-effnet v1b0\n\n-target_3(Sinan Calisir)\n\n-eva02 ( I dropped it)\n\nnow, image predictions are deactivated.\n\nand also dropped least important features because Farukcan Sağmaz who created the excellent work uses One Hot Encoder, so it creates a lot of columns, and it increases dimensionality and model's performances decreases\n\n","metadata":{}},{"cell_type":"markdown","source":"# IMPORT LIBRARIES","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport polars as pl\n\nfrom sklearn.model_selection import StratifiedGroupKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import VotingClassifier\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.over_sampling import RandomOverSampler\n\n\nimport lightgbm as lgb\nimport catboost as cb\nimport xgboost as xgb\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\n#downsampling techniques\n# they took long time, so we use RandomUnderSampler\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.under_sampling import ClusterCentroids\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.under_sampling import EditedNearestNeighbours\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom imblearn.under_sampling import NearMiss, TomekLinks\nfrom sklearn.impute import SimpleImputer\n\nimport time\nfrom sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif, VarianceThreshold\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:31:30.157539Z","iopub.execute_input":"2024-08-14T04:31:30.157982Z","iopub.status.idle":"2024-08-14T04:31:30.16773Z","shell.execute_reply.started":"2024-08-14T04:31:30.157954Z","shell.execute_reply":"2024-08-14T04:31:30.166784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IMAGE PREDICTIONS","metadata":{}},{"cell_type":"code","source":"#EFFNET V1B0\n!python /kaggle/input/isic-script-inference-effnetv1b0-f313ae/main.py /kaggle/input/isic-pytorch-training-baseline-image-only/AUROC0.5171_Loss0.3476_epoch35.bin\n!mv submission.csv submission_effnetv1b0.csv","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:31:30.169588Z","iopub.execute_input":"2024-08-14T04:31:30.169872Z","iopub.status.idle":"2024-08-14T04:31:39.450023Z","shell.execute_reply.started":"2024-08-14T04:31:30.169848Z","shell.execute_reply":"2024-08-14T04:31:39.44857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#TARGET 3\n!python /kaggle/input/isic-2024-pl-submission-script-and-preds/pl_submission.py\n!mv submission.csv submission_image3.csv","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:31:39.45181Z","iopub.execute_input":"2024-08-14T04:31:39.452166Z","iopub.status.idle":"2024-08-14T04:31:56.979641Z","shell.execute_reply.started":"2024-08-14T04:31:39.452134Z","shell.execute_reply":"2024-08-14T04:31:56.978224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#EVA02\n!python /kaggle/input/isic-script-inference-eva02/main.py /kaggle/input/isic-pytorch-training-baseline-eva02/AUROC0.5177_Loss0.2829_epoch7.bin\n!mv submission.csv submission_eva02.csv","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:31:56.981545Z","iopub.execute_input":"2024-08-14T04:31:56.981895Z","iopub.status.idle":"2024-08-14T04:32:06.614972Z","shell.execute_reply.started":"2024-08-14T04:31:56.98186Z","shell.execute_reply":"2024-08-14T04:32:06.613734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DATA PREPROCESS","metadata":{}},{"cell_type":"code","source":"root = Path('/kaggle/input/isic-2024-challenge')\n\ntrain_path = root / 'train-metadata.csv'\ntest_path = root / 'test-metadata.csv'\nsubm_path = root / 'sample_submission.csv'\n\nid_col = 'isic_id'\ntarget_col = 'target'\ngroup_col = 'patient_id'\n\nerr = 1e-5\nsampling_ratio = 0.01\nseed = 42\n\nnum_cols = [\n    'age_approx',                        # Approximate age of patient at time of imaging.\n    'clin_size_long_diam_mm',            # Maximum diameter of the lesion (mm).+\n    'tbp_lv_A',                          # A inside  lesion.+\n    'tbp_lv_Aext',                       # A outside lesion.+\n    'tbp_lv_B',                          # B inside  lesion.+\n    'tbp_lv_Bext',                       # B outside lesion.+ \n    'tbp_lv_C',                          # Chroma inside  lesion.+\n    'tbp_lv_Cext',                       # Chroma outside lesion.+\n    'tbp_lv_H',                          # Hue inside the lesion; calculated as the angle of A* and B* in LAB* color space. Typical values range from 25 (red) to 75 (brown).+\n    'tbp_lv_Hext',                       # Hue outside lesion.+\n    'tbp_lv_L',                          # L inside lesion.+\n    'tbp_lv_Lext',                       # L outside lesion.+\n    'tbp_lv_areaMM2',                    # Area of lesion (mm^2).+\n    'tbp_lv_area_perim_ratio',           # Border jaggedness, the ratio between lesions perimeter and area. Circular lesions will have low values; irregular shaped lesions will have higher values. Values range 0-10.+\n    'tbp_lv_color_std_mean',             # Color irregularity, calculated as the variance of colors within the lesion's boundary.\n    'tbp_lv_deltaA',                     # Average A contrast (inside vs. outside lesion).+\n    'tbp_lv_deltaB',                     # Average B contrast (inside vs. outside lesion).+\n    'tbp_lv_deltaL',                     # Average L contrast (inside vs. outside lesion).+\n    'tbp_lv_deltaLB',                    #\n    'tbp_lv_deltaLBnorm',                # Contrast between the lesion and its immediate surrounding skin. Low contrast lesions tend to be faintly visible such as freckles; high contrast lesions tend to be those with darker pigment. Calculated as the average delta LB of the lesion relative to its immediate background in LAB* color space. Typical values range from 5.5 to 25.+\n    'tbp_lv_eccentricity',               # Eccentricity.+\n    'tbp_lv_minorAxisMM',                # Smallest lesion diameter (mm).+\n    'tbp_lv_nevi_confidence',            # Nevus confidence score (0-100 scale) is a convolutional neural network classifier estimated probability that the lesion is a nevus. The neural network was trained on approximately 57,000 lesions that were classified and labeled by a dermatologist.+,++\n    'tbp_lv_norm_border',                # Border irregularity (0-10 scale); the normalized average of border jaggedness and asymmetry.+\n    'tbp_lv_norm_color',                 # Color variation (0-10 scale); the normalized average of color asymmetry and color irregularity.+\n    'tbp_lv_perimeterMM',                # Perimeter of lesion (mm).+\n    'tbp_lv_radial_color_std_max',       # Color asymmetry, a measure of asymmetry of the spatial distribution of color within the lesion. This score is calculated by looking at the average standard deviation in LAB* color space within concentric rings originating from the lesion center. Values range 0-10.+\n    'tbp_lv_stdL',                       # Standard deviation of L inside  lesion.+\n    'tbp_lv_stdLExt',                    # Standard deviation of L outside lesion.+\n    'tbp_lv_symm_2axis',                 # Border asymmetry; a measure of asymmetry of the lesion's contour about an axis perpendicular to the lesion's most symmetric axis. Lesions with two axes of symmetry will therefore have low scores (more symmetric), while lesions with only one or zero axes of symmetry will have higher scores (less symmetric). This score is calculated by comparing opposite halves of the lesion contour over many degrees of rotation. The angle where the halves are most similar identifies the principal axis of symmetry, while the second axis of symmetry is perpendicular to the principal axis. Border asymmetry is reported as the asymmetry value about this second axis. Values range 0-10.+\n    'tbp_lv_symm_2axis_angle',           # Lesion border asymmetry angle.+\n    'tbp_lv_x',                          # X-coordinate of the lesion on 3D TBP.+\n    'tbp_lv_y',                          # Y-coordinate of the lesion on 3D TBP.+\n    'tbp_lv_z',                          # Z-coordinate of the lesion on 3D TBP.+\n]\n\nnew_num_cols = [\n    'lesion_size_ratio',             # tbp_lv_minorAxisMM      / clin_size_long_diam_mm\n    'lesion_shape_index',            # tbp_lv_areaMM2          / tbp_lv_perimeterMM **2\n    'hue_contrast',                  # tbp_lv_H                - tbp_lv_Hext              abs\n    'luminance_contrast',            # tbp_lv_L                - tbp_lv_Lext              abs\n    'lesion_color_difference',       # tbp_lv_deltaA **2       + tbp_lv_deltaB **2 + tbp_lv_deltaL **2  sqrt  \n    'border_complexity',             # tbp_lv_norm_border      + tbp_lv_symm_2axis\n    'color_uniformity',              # tbp_lv_color_std_mean   / tbp_lv_radial_color_std_max\n\n    'position_distance_3d',          # tbp_lv_x **2 + tbp_lv_y **2 + tbp_lv_z **2  sqrt\n    'perimeter_to_area_ratio',       # tbp_lv_perimeterMM      / tbp_lv_areaMM2\n    'area_to_perimeter_ratio',       # tbp_lv_areaMM2          / tbp_lv_perimeterMM\n    'lesion_visibility_score',       # tbp_lv_deltaLBnorm      + tbp_lv_norm_color\n    'symmetry_border_consistency',   # tbp_lv_symm_2axis       * tbp_lv_norm_border\n    'consistency_symmetry_border',   # tbp_lv_symm_2axis       * tbp_lv_norm_border / (tbp_lv_symm_2axis + tbp_lv_norm_border)\n\n    'color_consistency',             # tbp_lv_stdL             / tbp_lv_Lext\n    'consistency_color',             # tbp_lv_stdL*tbp_lv_Lext / tbp_lv_stdL + tbp_lv_Lext\n    'size_age_interaction',          # clin_size_long_diam_mm  * age_approx\n    'hue_color_std_interaction',     # tbp_lv_H                * tbp_lv_color_std_mean\n    'lesion_severity_index',         # tbp_lv_norm_border      + tbp_lv_norm_color + tbp_lv_eccentricity / 3\n    'shape_complexity_index',        # border_complexity       + lesion_shape_index\n    'color_contrast_index',          # tbp_lv_deltaA + tbp_lv_deltaB + tbp_lv_deltaL + tbp_lv_deltaLBnorm\n\n    'log_lesion_area',               # tbp_lv_areaMM2          + 1  np.log\n    'normalized_lesion_size',        # clin_size_long_diam_mm  / age_approx\n    'mean_hue_difference',           # tbp_lv_H                + tbp_lv_Hext    / 2\n    'std_dev_contrast',              # tbp_lv_deltaA **2 + tbp_lv_deltaB **2 + tbp_lv_deltaL **2   / 3  np.sqrt\n    'color_shape_composite_index',   # tbp_lv_color_std_mean   + bp_lv_area_perim_ratio + tbp_lv_symm_2axis   / 3\n    'lesion_orientation_3d',         # tbp_lv_y                , tbp_lv_x  np.arctan2\n    'overall_color_difference',      # tbp_lv_deltaA           + tbp_lv_deltaB + tbp_lv_deltaL   / 3\n\n    'symmetry_perimeter_interaction',# tbp_lv_symm_2axis       * tbp_lv_perimeterMM\n    'comprehensive_lesion_index',    # tbp_lv_area_perim_ratio + tbp_lv_eccentricity + bp_lv_norm_color + tbp_lv_symm_2axis   / 4\n    'color_variance_ratio',          # tbp_lv_color_std_mean   / tbp_lv_stdLExt\n    'border_color_interaction',      # tbp_lv_norm_border      * tbp_lv_norm_color\n    'border_color_interaction_2',\n    'size_color_contrast_ratio',     # clin_size_long_diam_mm  / tbp_lv_deltaLBnorm\n    'age_normalized_nevi_confidence',# tbp_lv_nevi_confidence  / age_approx\n    'age_normalized_nevi_confidence_2',\n    'color_asymmetry_index',         # tbp_lv_symm_2axis       * tbp_lv_radial_color_std_max\n\n    'volume_approximation_3d',       # tbp_lv_areaMM2          * sqrt(tbp_lv_x**2 + tbp_lv_y**2 + tbp_lv_z**2)\n    'color_range',                   # abs(tbp_lv_L - tbp_lv_Lext) + abs(tbp_lv_A - tbp_lv_Aext) + abs(tbp_lv_B - tbp_lv_Bext)\n    'shape_color_consistency',       # tbp_lv_eccentricity     * tbp_lv_color_std_mean\n    'border_length_ratio',           # tbp_lv_perimeterMM      / pi * sqrt(tbp_lv_areaMM2 / pi)\n    'age_size_symmetry_index',       # age_approx              * clin_size_long_diam_mm * tbp_lv_symm_2axis\n    'index_age_size_symmetry',       # age_approx              * tbp_lv_areaMM2 * tbp_lv_symm_2axis\n]\n\ncat_cols = ['sex', 'anatom_site_general', 'tbp_tile_type', 'tbp_lv_location', 'tbp_lv_location_simple', 'attribution']\nnorm_cols = [f'{col}_patient_norm' for col in num_cols + new_num_cols]\nspecial_cols = ['count_per_patient']\nimage_cols = [\"target_3\",\"target_effnetv1b0\",\"target_eva02\"]\n#image_cols = [\"target_3\",\"target_effnetv1b0\"]\n\n#norm_cols += image_cols\nfeature_cols = num_cols + new_num_cols + cat_cols + norm_cols + special_cols","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:32:06.619222Z","iopub.execute_input":"2024-08-14T04:32:06.619542Z","iopub.status.idle":"2024-08-14T04:32:06.638566Z","shell.execute_reply.started":"2024-08-14T04:32:06.619506Z","shell.execute_reply":"2024-08-14T04:32:06.637385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_data(path):\n    return (\n        pl.read_csv(path)\n        .with_columns(\n            pl.col('age_approx').cast(pl.String).replace('NA', np.nan).cast(pl.Float64),\n        )\n        .with_columns(\n            pl.col(pl.Float64).fill_nan(pl.col(pl.Float64).median()), # You may want to impute test data with train\n        )\n        .with_columns(\n            lesion_size_ratio              = pl.col('tbp_lv_minorAxisMM') / pl.col('clin_size_long_diam_mm'),\n            lesion_shape_index             = pl.col('tbp_lv_areaMM2') / (pl.col('tbp_lv_perimeterMM') ** 2),\n            hue_contrast                   = (pl.col('tbp_lv_H') - pl.col('tbp_lv_Hext')).abs(),\n            luminance_contrast             = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs(),\n            lesion_color_difference        = (pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2).sqrt(),\n            border_complexity              = pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_symm_2axis'),\n            color_uniformity               = pl.col('tbp_lv_color_std_mean') / (pl.col('tbp_lv_radial_color_std_max') + err),\n        )\n        .with_columns(\n            position_distance_3d           = (pl.col('tbp_lv_x') ** 2 + pl.col('tbp_lv_y') ** 2 + pl.col('tbp_lv_z') ** 2).sqrt(),\n            perimeter_to_area_ratio        = pl.col('tbp_lv_perimeterMM') / pl.col('tbp_lv_areaMM2'),\n            area_to_perimeter_ratio        = pl.col('tbp_lv_areaMM2') / pl.col('tbp_lv_perimeterMM'),\n            lesion_visibility_score        = pl.col('tbp_lv_deltaLBnorm') + pl.col('tbp_lv_norm_color'),\n            combined_anatomical_site       = pl.col('anatom_site_general') + '_' + pl.col('tbp_lv_location'),\n            symmetry_border_consistency    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border'),\n            consistency_symmetry_border    = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_norm_border') / (pl.col('tbp_lv_symm_2axis') + pl.col('tbp_lv_norm_border')),\n        )\n        .with_columns(\n            color_consistency              = pl.col('tbp_lv_stdL') / pl.col('tbp_lv_Lext'),\n            consistency_color              = pl.col('tbp_lv_stdL') * pl.col('tbp_lv_Lext') / (pl.col('tbp_lv_stdL') + pl.col('tbp_lv_Lext')),\n            size_age_interaction           = pl.col('clin_size_long_diam_mm') * pl.col('age_approx'),\n            hue_color_std_interaction      = pl.col('tbp_lv_H') * pl.col('tbp_lv_color_std_mean'),\n            lesion_severity_index          = (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_eccentricity')) / 3,\n            shape_complexity_index         = pl.col('border_complexity') + pl.col('lesion_shape_index'),\n            color_contrast_index           = pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL') + pl.col('tbp_lv_deltaLBnorm'),\n        )\n        .with_columns(\n            log_lesion_area                = (pl.col('tbp_lv_areaMM2') + 1).log(),\n            normalized_lesion_size         = pl.col('clin_size_long_diam_mm') / pl.col('age_approx'),\n            mean_hue_difference            = (pl.col('tbp_lv_H') + pl.col('tbp_lv_Hext')) / 2,\n            std_dev_contrast               = ((pl.col('tbp_lv_deltaA') ** 2 + pl.col('tbp_lv_deltaB') ** 2 + pl.col('tbp_lv_deltaL') ** 2) / 3).sqrt(),\n            color_shape_composite_index    = (pl.col('tbp_lv_color_std_mean') + pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_symm_2axis')) / 3,\n            lesion_orientation_3d          = pl.arctan2(pl.col('tbp_lv_y'), pl.col('tbp_lv_x')),\n            overall_color_difference       = (pl.col('tbp_lv_deltaA') + pl.col('tbp_lv_deltaB') + pl.col('tbp_lv_deltaL')) / 3,\n        )\n        .with_columns(\n            symmetry_perimeter_interaction = pl.col('tbp_lv_symm_2axis') * pl.col('tbp_lv_perimeterMM'),\n            comprehensive_lesion_index     = (pl.col('tbp_lv_area_perim_ratio') + pl.col('tbp_lv_eccentricity') + pl.col('tbp_lv_norm_color') + pl.col('tbp_lv_symm_2axis')) / 4,\n            color_variance_ratio           = pl.col('tbp_lv_color_std_mean') / pl.col('tbp_lv_stdLExt'),\n            border_color_interaction       = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color'),\n            border_color_interaction_2     = pl.col('tbp_lv_norm_border') * pl.col('tbp_lv_norm_color') / (pl.col('tbp_lv_norm_border') + pl.col('tbp_lv_norm_color')),\n            size_color_contrast_ratio      = pl.col('clin_size_long_diam_mm') / pl.col('tbp_lv_deltaLBnorm'),\n            age_normalized_nevi_confidence = pl.col('tbp_lv_nevi_confidence') / pl.col('age_approx'),\n            age_normalized_nevi_confidence_2 = (pl.col('clin_size_long_diam_mm')**2 + pl.col('age_approx')**2).sqrt(),\n            color_asymmetry_index          = pl.col('tbp_lv_radial_color_std_max') * pl.col('tbp_lv_symm_2axis'),\n        )\n        .with_columns(\n            volume_approximation_3d        = pl.col('tbp_lv_areaMM2') * (pl.col('tbp_lv_x')**2 + pl.col('tbp_lv_y')**2 + pl.col('tbp_lv_z')**2).sqrt(),\n            color_range                    = (pl.col('tbp_lv_L') - pl.col('tbp_lv_Lext')).abs() + (pl.col('tbp_lv_A') - pl.col('tbp_lv_Aext')).abs() + (pl.col('tbp_lv_B') - pl.col('tbp_lv_Bext')).abs(),\n            shape_color_consistency        = pl.col('tbp_lv_eccentricity') * pl.col('tbp_lv_color_std_mean'),\n            border_length_ratio            = pl.col('tbp_lv_perimeterMM') / (2 * np.pi * (pl.col('tbp_lv_areaMM2') / np.pi).sqrt()),\n            age_size_symmetry_index        = pl.col('age_approx') * pl.col('clin_size_long_diam_mm') * pl.col('tbp_lv_symm_2axis'),\n            index_age_size_symmetry        = pl.col('age_approx') * pl.col('tbp_lv_areaMM2') * pl.col('tbp_lv_symm_2axis'),\n        )\n        .with_columns(\n            ((pl.col(col) - pl.col(col).mean().over('patient_id')) / (pl.col(col).std().over('patient_id') + err)).alias(f'{col}_patient_norm') for col in (num_cols + new_num_cols)\n        )\n        .with_columns(\n            count_per_patient = pl.col('isic_id').count().over('patient_id'),\n        )\n        .with_columns(\n            pl.col(cat_cols).cast(pl.Categorical),\n        )\n        .to_pandas()\n        .set_index(id_col)\n    )","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:32:06.640205Z","iopub.execute_input":"2024-08-14T04:32:06.640626Z","iopub.status.idle":"2024-08-14T04:32:06.670064Z","shell.execute_reply.started":"2024-08-14T04:32:06.640593Z","shell.execute_reply":"2024-08-14T04:32:06.669153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(df_train, df_test):\n    global cat_cols\n    \n    encoder = OneHotEncoder(sparse_output=False, dtype=np.int32, handle_unknown='ignore')\n    encoder.fit(df_train[cat_cols])\n    \n    new_cat_cols = [f'onehot_{i}' for i in range(len(encoder.get_feature_names_out()))]\n\n    df_train[new_cat_cols] = encoder.transform(df_train[cat_cols])\n    df_train[new_cat_cols] = df_train[new_cat_cols].astype('category')\n\n    df_test[new_cat_cols] = encoder.transform(df_test[cat_cols])\n    df_test[new_cat_cols] = df_test[new_cat_cols].astype('category')\n    \n    \n    # effnetv1b0\n    df_eff = pd.read_csv(\"/kaggle/input/isic-inference-effnetv1b0-for-training-data/train_effnetv1b0.csv\")\n  \n    df_train = df_train.reset_index(drop=True)\n    df_eff = df_eff.reset_index(drop=True)\n    \n    #upload effnetv1b0 train predictions values\n    df_train[\"target_effnetv1b0\"] = df_eff[\"target_effnetv1b0\"]\n    df_eff = pd.read_csv(\"submission_effnetv1b0.csv\")\n    \n    df_test = df_test.reset_index(drop=True)\n    df_eff = df_eff.reset_index(drop=True)\n    \n    #upload effnetv1b0 test predictions values\n    df_test[\"target_effnetv1b0\"] = df_eff[\"target\"]\n        \n    # target 3\n    df_image_3 = pd.read_csv(\"/kaggle/input/isic-2024-pl-submission-script-and-preds/train_preds.csv\")\n    \n    df_train = df_train.reset_index(drop=True)\n    df_image_3 = df_image_3.reset_index(drop=True)\n    \n    df_train[\"target_3\"] = df_image_3[\"pred\"]\n    \n    df_3 = pd.read_csv(\"submission_image3.csv\")\n    \n    df_test = df_test.reset_index(drop=True)\n    df_image_3 = df_image_3.reset_index(drop=True)\n    \n    df_test[\"target_3\"] = df_3[\"target\"]\n    \n    #eva02\n    df_eva = pd.read_csv(\"/kaggle/input/isic-inference-eva02-for-training-data/train_eva02.csv\")\n    \n    df_train = df_train.reset_index(drop=True)\n    df_eva = df_eva.reset_index(drop=True)\n    \n    df_eva = df_eva[[\"target_eva02\"]]\n    df_train[\"target_eva02\"] = df_eva[\"target_eva02\"]\n    \n    df_eva = pd.read_csv(\"submission_eva02.csv\")\n    \n    df_test = df_test.reset_index(drop=True)\n    df_eva = df_eva.reset_index(drop=True)\n    \n    df_test[\"target_eva02\"] = df_eva[\"target\"]\n\n\n    for col in cat_cols:\n        feature_cols.remove(col)\n\n    feature_cols.extend(new_cat_cols)\n    cat_cols = new_cat_cols\n    \n    return df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:32:06.671557Z","iopub.execute_input":"2024-08-14T04:32:06.672355Z","iopub.status.idle":"2024-08-14T04:32:06.686996Z","shell.execute_reply.started":"2024-08-14T04:32:06.672321Z","shell.execute_reply":"2024-08-14T04:32:06.686112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def custom_metric(estimator, X, y_true):\n    y_hat = estimator.predict_proba(X)[:, 1]\n    min_tpr = 0.80\n    max_fpr = abs(1 - min_tpr)\n    \n    v_gt = abs(y_true - 1)\n    v_pred = np.array([1.0 - x for x in y_hat])\n    \n    partial_auc_scaled = roc_auc_score(v_gt, v_pred, max_fpr=max_fpr)\n    partial_auc = 0.5 * max_fpr**2 + (max_fpr - 0.5 * max_fpr**2) / (1.0 - 0.5) * (partial_auc_scaled - 0.5)\n    \n    return partial_auc","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:32:06.688293Z","iopub.execute_input":"2024-08-14T04:32:06.68871Z","iopub.status.idle":"2024-08-14T04:32:06.699766Z","shell.execute_reply.started":"2024-08-14T04:32:06.688659Z","shell.execute_reply":"2024-08-14T04:32:06.698991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = read_data(train_path)\ndf_test = read_data(test_path)\ndf_subm = pd.read_csv(subm_path, index_col=id_col)\n\ndf_train, df_test = preprocess(df_train, df_test)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:32:06.700875Z","iopub.execute_input":"2024-08-14T04:32:06.701178Z","iopub.status.idle":"2024-08-14T04:32:24.186815Z","shell.execute_reply.started":"2024-08-14T04:32:06.701155Z","shell.execute_reply":"2024-08-14T04:32:24.185683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***drop least important features with feature importance(code at the last of the notebook before test predictions)***","metadata":{}},{"cell_type":"code","source":"#they are detected at the first run\nleast_important_features = ['onehot_32', 'onehot_6', 'onehot_33', 'onehot_30', 'onehot_26', 'onehot_22', 'onehot_36', 'onehot_4']\n#they are detected after the least_important_features are removed and it has increased cv score also so I add it\n#least_important_features_2 = ['onehot_17', 'onehot_42', 'onehot_29', 'onehot_13', 'onehot_25']\n#least_important_features += least_important_features_2\ndf_train.drop(columns =least_important_features,inplace = True)\nfor feature in least_important_features:\n    cat_cols.remove(feature)\n    feature_cols.remove(feature)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:32:24.188199Z","iopub.execute_input":"2024-08-14T04:32:24.188578Z","iopub.status.idle":"2024-08-14T04:32:24.410841Z","shell.execute_reply.started":"2024-08-14T04:32:24.188547Z","shell.execute_reply":"2024-08-14T04:32:24.409958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL INITIALIZATION","metadata":{}},{"cell_type":"code","source":"import copy\nfeature_cols_without_image_cols = copy.copy(feature_cols)\nfeature_cols += image_cols\n\nlgb_params = {\n    'objective':        'binary',\n    'verbosity':        -1,\n    'n_iter':           178,\n    'boosting_type':    'gbdt',\n    'random_state':     seed,\n    'lambda_l1':        0.08758718919397321, \n    'lambda_l2':        0.0039689175176025465, \n    'learning_rate':  0.14190644, \n    'max_depth':        5, \n    'num_leaves':       100, \n    'colsample_bytree': 0.20227227, \n    'colsample_bynode': 0.41808809, \n    'bagging_fraction': 0.01196196, \n    'bagging_freq':     4, \n    'min_data_in_leaf': 85, \n    'scale_pos_weight': 2.7984184778875543,\n}\n\n\nsampling_ratio = 0.01\nseed =42\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass skPlumberBase(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return self\n    \nclass SelectColumns(skPlumberBase):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def transform(self, X):\n        return X[self.columns]\n\nlgb_model = Pipeline([\n    ('sampler_1', RandomOverSampler(sampling_strategy= 0.003 , random_state=seed)),\n    ('sampler_2', RandomUnderSampler(sampling_strategy=sampling_ratio, random_state=seed)),\n    ('filter', SelectColumns(feature_cols_without_image_cols)),\n    ('classifier', lgb.LGBMClassifier(**lgb_params)),\n])","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:32:24.411897Z","iopub.execute_input":"2024-08-14T04:32:24.412165Z","iopub.status.idle":"2024-08-14T04:32:24.422527Z","shell.execute_reply.started":"2024-08-14T04:32:24.412137Z","shell.execute_reply":"2024-08-14T04:32:24.42154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb_params = {\n    'loss_function':     'Logloss',\n    'iterations':        239,\n    'verbose':           False,\n    'random_state':      seed,\n    'max_depth':         7, \n    'learning_rate':     0.08775605, \n    'scale_pos_weight':  2.6149345838209532, \n    'l2_leaf_reg':       6.216113851699493, \n    'subsample':         0.6249261779711819, \n    'min_data_in_leaf':  24,\n    'cat_features':      cat_cols,\n}\ncb_model = Pipeline([\n    ('sampler_1', RandomOverSampler(sampling_strategy= 0.003 , random_state=seed)),\n    ('sampler_2', RandomUnderSampler(sampling_strategy=sampling_ratio, random_state=seed)),\n    ('classifier', cb.CatBoostClassifier(**cb_params)),\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:32:24.423828Z","iopub.execute_input":"2024-08-14T04:32:24.424541Z","iopub.status.idle":"2024-08-14T04:32:24.433708Z","shell.execute_reply.started":"2024-08-14T04:32:24.424468Z","shell.execute_reply":"2024-08-14T04:32:24.432715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb_params = {\n    'enable_categorical': True,\n    'tree_method':        'hist',\n    'random_state':       seed,\n    'learning_rate':      0.08501257473292347, \n    'lambda':             8.879624125465703, \n    'alpha':              0.6779926606782505, \n    'max_depth':          6, \n    'subsample':          0.6012681388711075, \n    'colsample_bytree':   0.8437772277074493, \n    'colsample_bylevel':  0.5476090898823716, \n    'colsample_bynode':   0.9928601203635129, \n    'scale_pos_weight':   3.29440313334688,\n}\n\nxgb_model = Pipeline([\n    ('sampler_1', RandomOverSampler(sampling_strategy= 0.003 , random_state=seed)),\n    ('sampler_2', RandomUnderSampler(sampling_strategy=sampling_ratio, random_state=seed)),\n    ('classifier', xgb.XGBClassifier(**xgb_params)),\n])\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:32:24.438313Z","iopub.execute_input":"2024-08-14T04:32:24.438678Z","iopub.status.idle":"2024-08-14T04:32:24.448217Z","shell.execute_reply.started":"2024-08-14T04:32:24.43865Z","shell.execute_reply":"2024-08-14T04:32:24.447221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"estimator = VotingClassifier([\n    ('lgb', lgb_model), ('cb', cb_model), ('xgb', xgb_model),\n], voting='soft')","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:32:24.44952Z","iopub.execute_input":"2024-08-14T04:32:24.449902Z","iopub.status.idle":"2024-08-14T04:32:24.458983Z","shell.execute_reply.started":"2024-08-14T04:32:24.449866Z","shell.execute_reply":"2024-08-14T04:32:24.4582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CROSS VALIDATION","metadata":{}},{"cell_type":"code","source":"\n\nX = df_train[feature_cols]\ny = df_train[target_col]\ngroups = df_train[group_col]\ncv = StratifiedGroupKFold(5, shuffle=True, random_state=seed)\n\nval_score = cross_val_score(\n    estimator=estimator, \n    X=X, y=y, \n    cv=cv, \n    groups=groups,\n    scoring=custom_metric,\n)\n\nnp.mean(val_score), val_score\n\n\n# -------------------CV SCORES------------------------\n# RUS ==RandomUnderSampler(0.001) , ROS ==RandomOverSampler(0.003)\n# DROP NOTHING ----------------------> (0.18775) (RUS + image features(effnet+ target3))\n# DROP least_important_features -----> (0.18747) (RUS + image features(effnet+target3))\n# DROP NOTHING ----------------------> (0.17000) (ROS+RUS + no image features)\n# DROP least_important_features -----> (0.17124) (ROS+RUS + no image features)\n# DROP least_important_features+2 ---> (0.17160) (ROS+RUS + no image features)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T04:32:24.460274Z","iopub.execute_input":"2024-08-14T04:32:24.460614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# HYPERPARAMETER TUNING","metadata":{}},{"cell_type":"code","source":"DO_TUNING = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lgb_objective(trial):\n    params = {\n        'objective':         'binary',\n        'verbosity':         -1,\n        'n_iter': 200,\n        'boosting_type':  'gbdt',\n        'lambda_l1':         trial.suggest_float('lambda_l1', 1e-3, 10.0, log=True),\n        'lambda_l2':         trial.suggest_float('lambda_l2', 1e-3, 10.0, log=True),\n        'learning_rate':     trial.suggest_float('learning_rate', 1e-2, 1e-1, log=True),\n        'max_depth':         trial.suggest_int('max_depth', 4, 8),\n        'num_leaves':        trial.suggest_int('num_leaves', 16, 256),\n        'colsample_bytree':  trial.suggest_float('colsample_bytree', 0.4, 1.0),\n        'colsample_bynode':  trial.suggest_float('colsample_bynode', 0.4, 1.0),\n        'bagging_fraction':  trial.suggest_float('bagging_fraction', 0.4, 1.0),\n        'bagging_freq':      trial.suggest_int('bagging_freq', 1, 7),\n        'min_data_in_leaf':  trial.suggest_int('min_data_in_leaf', 5, 100),\n        'scale_pos_weight' : trial.suggest_float('scale_pos_weight', 0.8, 4.0),\n    }\n\n    estimator = Pipeline([\n        ('sampler', RandomUnderSampler(sampling_strategy=sampling_ratio)),\n        ('classifier', lgb.LGBMClassifier(**params)),\n    ])\n\n    X = df_train[feature_cols]\n    y = df_train[target_col]\n    groups = df_train[group_col]\n    cv = StratifiedGroupKFold(5, shuffle=True)\n\n    val_score = cross_val_score(\n        estimator=estimator, \n        X=X, y=y, \n        cv=cv, \n        groups=groups,\n        scoring=custom_metric,\n    )\n\n    return np.mean(val_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cb_objective(trial):\n    params = {\n        'loss_function':     'Logloss',\n        'iterations':        200,\n        'verbose':           False,\n        'random_state':      seed,\n        'learning_rate':     trial.suggest_float('learning_rate', 1e-2, 1e-1, log=True),\n        'max_depth':         trial.suggest_int('max_depth', 4, 8),\n        'l2_leaf_reg':       trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n        'subsample':         trial.suggest_float('subsample', 0.4, 1.0),\n        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.4, 1.0),\n        'min_data_in_leaf':  trial.suggest_int('min_data_in_leaf', 5, 100),\n        'scale_pos_weight':  trial.suggest_float('scale_pos_weight', 0.8, 4.0),\n        'bootstrap_type':    'Bayesian',  # Optional: depending on your use case, you may want to tune this as well\n    }\n\n    estimator = Pipeline([\n        ('sampler', RandomUnderSampler(sampling_strategy=sampling_ratio)),\n        ('classifier', cb.CatBoostClassifier(**params)),\n    ])\n\n    X = df_train[feature_cols]\n    y = df_train[target_col]\n    groups = df_train[group_col]\n    cv = StratifiedGroupKFold(5, shuffle=True)\n\n    val_score = cross_val_score(\n        estimator=estimator, \n        X=X, y=y, \n        cv=cv, \n        groups=groups,\n        scoring=custom_metric,\n    )\n\n    return np.mean(val_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def xgb_objective(trial):\n    params = {\n        'objective':          'binary:logistic',\n        'n_estimators':       200,\n        'tree_method':        'hist',\n        'random_state':       seed,\n        'learning_rate':      trial.suggest_float('learning_rate', 1e-2, 1e-1, log=True),\n        'max_depth':          trial.suggest_int('max_depth', 4, 8),\n        'lambda':             trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n        'alpha':              trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n        'subsample':          trial.suggest_float('subsample', 0.4, 1.0),\n        'colsample_bytree':   trial.suggest_float('colsample_bytree', 0.4, 1.0),\n        'colsample_bynode':   trial.suggest_float('colsample_bynode', 0.4, 1.0),\n        'scale_pos_weight':   trial.suggest_float('scale_pos_weight', 0.8, 4.0),\n    }\n\n    estimator = Pipeline([\n        ('sampler', RandomUnderSampler(sampling_strategy=sampling_ratio)),\n        ('classifier', xgb.XGBClassifier(**params)),\n    ])\n\n    X = df_train[feature_cols]\n    y = df_train[target_col]\n    groups = df_train[group_col]\n    cv = StratifiedGroupKFold(5, shuffle=True)\n\n    val_score = cross_val_score(\n        estimator=estimator, \n        X=X, y=y, \n        cv=cv, \n        groups=groups,\n        scoring=custom_metric,\n    )\n\n    return np.mean(val_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DO_TUNING:\n    # LightGBM\n    start_time = time.time()\n    study_lgb = optuna.create_study(direction='maximize', sampler=TPESampler(seed=seed))\n    study_lgb.optimize(objective_lgb, n_trials=100)\n    end_time = time.time()\n    elapsed_time_lgb = end_time - start_time\n    print(f\"LightGBM tuning took {elapsed_time_lgb:.2f} seconds.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DO_TUNING:\n    # CatBoost\n    start_time = time.time()\n    study_cb = optuna.create_study(direction='maximize', sampler=TPESampler(seed=seed))\n    study_cb.optimize(objective_cb, n_trials=100)\n    end_time = time.time()\n    elapsed_time_cb = end_time - start_time\n    print(f\"CatBoost tuning took {elapsed_time_cb:.2f} seconds.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DO_TUNING:\n    # XGBoost\n    start_time = time.time()\n    study_xgb = optuna.create_study(direction='maximize', sampler=TPESampler(seed=seed))\n    study_xgb.optimize(objective_xgb, n_trials=100)\n    end_time = time.time()\n    elapsed_time_xgb = end_time - start_time\n    print(f\"XGBoost tuning took {elapsed_time_xgb:.2f} seconds.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DO_TUNING:\n    # Print best parameters for each study\n    print(\"Best LGBM trial:\", study_lgb.best_trial)\n    print(\"Best CatBoost trial:\", study_cb.best_trial)\n    print(\"Best XGBoost trial:\", study_xgb.best_trial)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TRAINING","metadata":{}},{"cell_type":"code","source":"X, y = df_train[feature_cols], df_train[target_col]\n\nestimator.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOOKING FOR FEATURE IMPORTANCE(lgb + xgb)","metadata":{}},{"cell_type":"code","source":"DO_FEATURE_IMPORTANCE_MODELS = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DO_FEATURE_IMPORTANCE_MODELS:\n    lgb_model = estimator.named_estimators_['lgb'].named_steps['classifier']\n    lgb_feature_importance = lgb_model.booster_.feature_importance(importance_type='gain')\n    lgb_feature_importance_df = pd.DataFrame({\n        'feature': X.columns,\n        'importance': lgb_feature_importance\n    }).sort_values(by='importance', ascending=False)\n\n\n    xgb_model = estimator.named_estimators_['xgb'].named_steps['classifier']\n    xgb_feature_importance = xgb_model.get_booster().get_score(importance_type='weight')\n    xgb_feature_importance_df = pd.DataFrame({\n        'feature': xgb_feature_importance.keys(),\n        'importance': xgb_feature_importance.values()\n    }).sort_values(by='importance', ascending=False)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DO_FEATURE_IMPORTANCE_MODELS:\n\n    print(lgb_feature_importance_df)\n    print(xgb_feature_importance_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LEAST IMPORTANT FEATURES","metadata":{}},{"cell_type":"code","source":"if DO_FEATURE_IMPORTANCE_MODELS:\n\n    # Assuming lgb_feature_importance_df is already created and contains the feature importances\n    least_important_lgb = lgb_feature_importance_df.sort_values(by='importance').head(24)\n\n    print(\"Least Important Features in LightGBM:\")\n    print(least_important_lgb)\n\n    # Convert the xgb_feature_importance to a DataFrame for easier manipulation\n    least_important_xgb = xgb_feature_importance_df.sort_values(by = \"importance\").head(6)\n\n\n    print(\"\\nLeast Important Features in XGBoost:\")\n    print(least_important_xgb)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DO_FEATURE_IMPORTANCE_MODELS:\n\n    # Extract the least important feature names from both LightGBM and XGBoost\n    least_important_lgb_features = least_important_lgb['feature'].tolist()\n    least_important_xgb_features = least_important_xgb['feature'].tolist()\n\n    # Find the intersection of the two lists\n    common_least_important_features = list(set(least_important_lgb_features) & set(least_important_xgb_features))\n\n    print(\"Common Least Important Features in Both LightGBM and XGBoost:\")\n    print(common_least_important_features)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOOKING FOR FEATURE IMPORTANCE(Tests)","metadata":{}},{"cell_type":"code","source":"DO_FEATURE_IMPORTANCE_TEST = False","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DO_FEATURE_IMPORTANCE_TEST:\n    X = df_train[feature_cols]\n    y = df_train[target_col]\n\n    # Separate continuous and categorical features\n    continuous_features = num_cols + norm_cols + new_num_cols\n    # Fill null values of continuous features with their median values\n    X[continuous_features] = X[continuous_features].apply(lambda x: x.fillna(x.median()))\n\n\n    # Correlation Matrix for continuous features\n    corr_matrix = df_train[continuous_features + ['target']].corr()\n    threshold = 0.01\n    relevant_features_corr = corr_matrix[abs(corr_matrix['target']) > threshold].index\n    selected_features_corr = relevant_features_corr.drop('target')\n    print(\"Selected continuous features based on correlation threshold:\")\n    print(selected_features_corr)\n    print(len(selected_features_corr))\n\n    # Chi-Square Test for categorical features\n    chi2_selector = SelectKBest(chi2, k=15)\n    chi2_selector.fit_transform(X[cat_cols], y)\n    selected_features_chi2 = X[cat_cols].columns[chi2_selector.get_support()]\n    print(\"Selected categorical features based on Chi-Square Test:\")\n    print(selected_features_chi2)\n\n    # Mutual Information for all features\n    mi_selector = SelectKBest(mutual_info_classif, k=15)\n    mi_selector.fit_transform(X, y)\n    selected_features_mi = X.columns[mi_selector.get_support()]\n    print(\"Selected features based on Mutual Information:\")\n    print(selected_features_mi)\n\n    # Variance Threshold for continuous features\n    threshold = 0.05\n    var_threshold = VarianceThreshold(threshold=threshold)\n    var_threshold.fit_transform(X[continuous_features])\n    selected_features_var = X[continuous_features].columns[var_threshold.get_support()]\n    print(\"Selected continuous features based on Variance Threshold:\")\n    print(selected_features_var)\n    print(len(selected_features_var))\n\n\n    # Combine all selected features\n    selected_features_all = set(selected_features_corr) | set(selected_features_chi2) | set(selected_features_mi) | set(selected_features_var)\n\n    # Original features\n    original_features = set(X.columns)\n\n    # Find features not selected by any method\n    least_selected_features = original_features - selected_features_all\n\n    boosting_selected_features = set()\n\n    least_selected_features_list = list(least_selected_features | boosting_selected_features)\n\n    print(least_selected_features)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# TEST PREDICTION","metadata":{}},{"cell_type":"code","source":"df_subm['target'] = estimator.predict_proba(df_test[feature_cols])[:, 1]\n\ndf_subm.to_csv('submission.csv')\ndf_subm.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}